{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805e4b5d-944b-44fc-b6b2-261f5f652ad9",
   "metadata": {},
   "source": [
    "# **BBM409 ASSIGNMENT 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabd68a-579f-481a-98b1-f730b07b9237",
   "metadata": {},
   "source": [
    "        Eylül TUNCEL - 21727801\n",
    "        Emre KÖSEN   - 21727498"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294e7a6-5e30-48b1-859b-20a6c4406c85",
   "metadata": {},
   "source": [
    "For this assignment, we implement a Neural Network and CNN to classify the examples on the Animal Classification Dataset mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb1171-da46-4a7d-8dc3-9da22acb3ff3",
   "metadata": {},
   "source": [
    "## **PART 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fdc2d3-f434-4477-9241-b437a8b2bcef",
   "metadata": {},
   "source": [
    "Below you can see the implementation of our neural network. For Part 1, we implement changing layers of neural network and run experiments on Animal Classification Dataset. We changed parameters (activation func, learning rate, batch size) and put the results to the report.\n",
    "Changing parameters:\n",
    "* **Hidden Layer Count :** \n",
    "    * 0\n",
    "    * 1  \n",
    "    * 2 \n",
    "* **Activation Functions :** \n",
    "    * sigmoid\n",
    "    * tanh \n",
    "    * relu\n",
    "* **Batch Size :** \n",
    "    * 16\n",
    "    * 32\n",
    "    * 64 \n",
    "    * 128\n",
    "* **Learning rate :** \n",
    "    * 0.02\n",
    "    * 0.01 \n",
    "    * 0.005\n",
    "    \n",
    "As an inputs we gave all pixels of each images. Which means 2500 pixels are given as x0,x1,x2,..,x2500 as the input layer. We have 2500 neurons in the input layer.\n",
    "\n",
    "\n",
    "<img src=https://ml4a.github.io/images/figures/mnist_1layer.png width=\"300\"> <img src=https://ml4a.github.io/images/figures/weights_analogy_2.png width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e6bce-ec42-4e2a-b3c8-7dce31c76bf4",
   "metadata": {},
   "source": [
    "<img src=https://www.tibco.com/sites/tibco/files/media_entity/2021-05/neutral-network-diagram.svg width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24bbb0-4a24-400c-bafd-b6c2a151403f",
   "metadata": {},
   "source": [
    "We give the changing parameters as a global variables to the functions. If you want to change some parameter, you should change it in the below code part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b81faa6-81d9-4169-98bf-0212a6ca6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pixel count in one side\n",
    "img_size = 50\n",
    "# total pixels in all input images (as features)\n",
    "n = img_size * img_size  \n",
    "c = 10  # number of classes\n",
    "\n",
    "# changing inputs given as global variables \n",
    "hidden_layer_count = 0\n",
    "learning_rate = 0.005\n",
    "epoch = 2\n",
    "batch_size = 128\n",
    "activation_func = \"sigmoid\"\n",
    "# activation_func = \"tan\"\n",
    "# activation_func = \"relu\"\n",
    "\n",
    "# calculate neuron size for each layer with code logic\n",
    "neuron_numbers = [n]  # as a start only input neurons are added \n",
    "\n",
    "# split the train-validation-test sets with those image counts\n",
    "# for a better split in numbers we choose (%78 train - %11 validation - %11 test)\n",
    "total_train_image_count = 20480\n",
    "total_validation_image_count = 2816\n",
    "total_test_image_count = 2816"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77245a6d-9432-417e-8967-42c93c26a98b",
   "metadata": {},
   "source": [
    "### **Initializing Parameters**\n",
    "Before starting to train our neural network, we should initialize some parameters like the neuron size of each layer and the initial weight/bias matrices for each layer.\n",
    "For choosing number of neurons in each layer we use this formula (we think fits best to our data): \n",
    "\n",
    "$\\sqrt{m+n}$ , where m=inout neurons, n=output neurons\n",
    "\n",
    "Initial weight matrix is generated with random numbers, and bias matrix generated with all zeros with respect to to the number of neurons in each layer.\n",
    "\n",
    "\n",
    "<img src=https://ml4a.github.io/images/figures/rolled_weights_mnist_0.png width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fc8f9e-0c4a-465f-9701-29509ac58a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    # parameters is the dictionary for weights and biases\n",
    "    parameters = {}\n",
    "    np.random.seed(101)\n",
    "\n",
    "    weight = []\n",
    "    bias = []\n",
    "\n",
    "    # initializing neuron numbers for each hidden layer by a formula given above\n",
    "    for i in range(hidden_layer_count):\n",
    "        x = round(math.sqrt(neuron_numbers[i] + c))\n",
    "        neuron_numbers.append(x)\n",
    "\n",
    "    # for each hidden layer there will be weights and biases connected that specific layer\n",
    "    for i in range(hidden_layer_count):\n",
    "        wei = np.random.randn(neuron_numbers[i + 1], neuron_numbers[i]) * 0.0001\n",
    "        bia = np.zeros((neuron_numbers[i + 1], 1))\n",
    "        weight.append(wei)\n",
    "        bias.append(bia)\n",
    "\n",
    "    wei = np.random.randn(c, neuron_numbers[hidden_layer_count]) * 0.0001\n",
    "    bia = np.zeros((c, 1))\n",
    "    weight.append(wei)\n",
    "    bias.append(bia)\n",
    "\n",
    "    parameters[\"w\"] = weight\n",
    "    parameters[\"b\"] = bias\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d34489-916b-437a-8de2-a154b0497e72",
   "metadata": {},
   "source": [
    "The datasets x and y are read from the pixels/images and creating an numpy array with the pixels. \n",
    "In x and y, every column represents an image. And batch size of images come together side by side to create x_train numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b589e94e-a038-4848-b5ea-8baa27a0ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train(data, x_train, y_train):\n",
    "    x_train = np.zeros((n, batch_size))\n",
    "    y_train = np.zeros((c, batch_size))\n",
    "\n",
    "    # initializing \n",
    "    for i in range(x_train.shape[0]):\n",
    "        for j in range(x_train.shape[1]):\n",
    "            x_train[i][j] = data[j][0][i]\n",
    "\n",
    "    for i in range(y_train.shape[1]):\n",
    "        class_index = data[i][1]\n",
    "        y_train[class_index][i] = 1\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655574d-ba5f-4760-acaa-2a87776d5394",
   "metadata": {},
   "source": [
    "### **Activation Functions**\n",
    "\n",
    "1. **Sigmoid :**\n",
    "\n",
    "   Sigmoid function is known as the logistic function which helps to normalize the output of any input in the range between 0 to 1.  The sigmoid functions formula is : \n",
    "  \n",
    "    $y=\\frac{1}{1+ e^(-x)}$\n",
    "   \n",
    "   <img src=https://www.aitude.com/wp-content/uploads/2020/08/sigmoid.png width=300>\n",
    "2. **Tanh :**\n",
    "\n",
    "    Tanh Activation function is superior then the Sigmoid Activation function because the range of this activation function is higher than the sigmoid activation function. It ranges between -1 to 1.Here negative values are also considered.\n",
    "    \n",
    "   $y=tanh(x)$\n",
    "   \n",
    "    <img src=https://www.aitude.com/wp-content/uploads/2020/08/tanh-graph-aitude-768x423.png width=300>\n",
    "    \n",
    "3. **Relu :**\n",
    "\n",
    "    ReLu is the best and most advanced activation function right now compared to the sigmoid and TanH. Here all the negative values are converted into the 0 so there are no negative values are available, but positive values could go to infinity.\n",
    "    \n",
    "    $y=max(0,x)$\n",
    "    \n",
    "    <img src=https://www.aitude.com/wp-content/uploads/2020/08/relu-activation.png width=300>\n",
    "    \n",
    "\n",
    "4. **Softmax :**\n",
    "\n",
    "    The softmax activation function is for the output layer of the neural network. It’s commonly used in multi-class learning problems where a set of features can be related to one-of-K classes. The values of softmax ranges between 0 to 1.\n",
    "    \n",
    "    $y=\\frac{e^x}{\\sum{e^x}}$\n",
    "    \n",
    "    <img src=https://qph.fs.quoracdn.net/main-qimg-5c7cbb4b9fa300ac1de0f1dc3568fa3c width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1401fbed-daa0-4dd1-8e6a-1f1c14e5ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / np.sum(expX, axis=0)\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return 1 - np.power(np.tanh(x), 2)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.array(x > 0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a953a8-688e-4a19-a4e5-3786d98c2525",
   "metadata": {},
   "source": [
    "### **Forward Propagation**\n",
    "\n",
    "In forward propagation the input data is fed in the forward direction through the network. Each hidden layer accepts the input data, processes it as per the activation function and passes to the next layer.\n",
    "In forward propagation, firstly we have x matrix and parameters dictionary(which have weights and biases).\n",
    "In this part we need to calculate the numbers inside each neuron in each layer. For this purpose we use activation functions.The formula for calculatinons in each layer:\n",
    "\n",
    "$ Z_i = Weight_i * X_i + Bias_i $ \n",
    "\n",
    "\n",
    "$ A_i = activationFunction( Z_i ) $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43b619a-1b7e-4c89-8542-27dcf3cdaa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, parameters):\n",
    "    w = []\n",
    "    b = []\n",
    "    for i in range(hidden_layer_count + 1):\n",
    "        w.append(parameters['w'][i])\n",
    "        b.append(parameters['b'][i])\n",
    "\n",
    "    a = []\n",
    "    z = []\n",
    "    for i in range(hidden_layer_count + 1):\n",
    "        if i == 0:  # first\n",
    "            zi = np.dot(w[i], x) + b[i]\n",
    "            z.append(zi)\n",
    "        else:\n",
    "            zi = np.dot(w[i], a[i - 1]) + b[i]\n",
    "            z.append(zi)\n",
    "\n",
    "        if i == hidden_layer_count:  # last element\n",
    "            ai = softmax(zi)\n",
    "            a.append(ai)\n",
    "        else:\n",
    "            if activation_func == \"sigmoid\":\n",
    "                ai = sigmoid(zi)\n",
    "            elif activation_func == \"tan\":\n",
    "                ai = tanh(zi)\n",
    "            else:\n",
    "                ai = relu(zi)\n",
    "            a.append(ai)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6584435-2b98-4ac7-aba0-8ed5379afa8e",
   "metadata": {},
   "source": [
    "### **Loss Function**\n",
    "\n",
    "In our assignment, the loss function is \"Negative Log Likelihood\". In output layer, we have softmax as an activation function and it gives us probabilities of each class. In negative log likelihood loss function we take only the actual class's probability to calculate. And the cost is calculated by taking mean value of all the loss values in the batch. The formula of negative log likelihood is :\n",
    "\n",
    "$ Loss = - (log(P_i)) $ \n",
    "\n",
    "$ Cost =  \\frac{1}{m} * \\sum{Loss}$ \n",
    "\n",
    "<img src=https://ljvmiranda921.github.io/assets/png/cs231n-ann/neg_log.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ef724e-3312-435c-b972-03482b1db376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(softmax_layer, y):\n",
    "    mx = y.shape[1]\n",
    "    loss = 0\n",
    "    for j in range(softmax_layer.shape[1]):\n",
    "        for i in range(softmax_layer.shape[0]):\n",
    "            if y[i][j] == 1:\n",
    "                probability = softmax_layer[i][j]\n",
    "                loss += (-(math.log(probability)))\n",
    "    cost = (1 / mx) * loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78ab43-be4a-4e72-aa29-00229f25f6f9",
   "metadata": {},
   "source": [
    "### **Backward Propagation**\n",
    "\n",
    "Backward propagation refers to the method of calculating the gradient of neural network parameters. The method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters. After the values are calculated, update all parameters with it to actually back propagate. With each batch, the weight of the each neuron changes according to the output and makes it creates the learning process. The calculations are as follows:\n",
    "\n",
    "* **For output layer (softmax):**\n",
    "\n",
    "$ Dz_i = DerivativeSoftmax(Z_i) = (A_p - Y) $\n",
    "\n",
    "* **For other layers:**\n",
    "\n",
    "$ Dz_i = W_2^T * Dz_p * derivative(Z_i) $\n",
    "\n",
    "* **The calculationf with derivatives:**\n",
    "\n",
    "$ Dw_i = \\frac{1}{m} * Dz_i * A_1^T$  \n",
    "\n",
    "$ Db_i = \\frac{1}{m} * sum(Dz_i ,1) $ \n",
    "\n",
    "$ W_i = W_i - (learning rate) * Dw_i $ \n",
    "\n",
    "$ B_i = B_i - (learning rate) * Db_i $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7029e36-d7aa-453c-8387-b4d925182518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x, y, parameters, forward_cache):\n",
    "    w = parameters[\"w\"].copy()\n",
    "    w.reverse()\n",
    "    a = forward_cache\n",
    "    a.reverse()\n",
    "    a.append(x)\n",
    "\n",
    "    dz = []\n",
    "    dw = []\n",
    "    db = []\n",
    "\n",
    "    m = batch_size\n",
    "\n",
    "    dz_output = (a[0] - y)\n",
    "    dw_output = (1 / m) * np.dot(dz_output, a[1].T)\n",
    "    db_output = (1 / m) * np.sum(dz_output, axis=1, keepdims=True)\n",
    "\n",
    "    dz.append(dz_output)\n",
    "    dw.append(dw_output)\n",
    "    db.append(db_output)\n",
    "\n",
    "    for i in range(hidden_layer_count):\n",
    "        if activation_func == \"sigmoid\":\n",
    "            dz_i = np.dot(w[i].T, dz[i]) * derivative_sigmoid(a[i + 1])\n",
    "        elif activation_func == \"tan\":\n",
    "            dz_i = np.dot(w[i].T, dz[i]) * derivative_tanh(a[i + 1])\n",
    "        else:\n",
    "            dz_i = np.dot(w[i].T, dz[i]) * derivative_relu(a[i + 1])\n",
    "        dw_i = (1 / m) * np.dot(dz_i, a[i + 2].T)\n",
    "        db_i = (1 / m) * np.sum(dz_i, axis=1, keepdims=True)\n",
    "        dz.append(dz_i)\n",
    "        dw.append(dw_i)\n",
    "        db.append(db_i)\n",
    "\n",
    "    dz.reverse()\n",
    "    dw.reverse()\n",
    "    db.reverse()\n",
    "\n",
    "    for i in range(hidden_layer_count + 1):\n",
    "        parameters['w'][i] = parameters['w'][i] - (learning_rate * dw[i])\n",
    "        parameters['b'][i] = parameters['b'][i] - (learning_rate * db[i])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b323f2-75d5-4d20-99ae-cd152dc18c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(softmax_layer, y):\n",
    "    true_count = 0\n",
    "    for j in range(softmax_layer.shape[1]):\n",
    "        max_prob = -1\n",
    "        index = -1\n",
    "        for i in range(softmax_layer.shape[0]):\n",
    "            if max_prob < softmax_layer[i][j]:\n",
    "                max_prob = softmax_layer[i][j]\n",
    "                index = i\n",
    "        if y[index][j] == 1:\n",
    "            true_count += 1\n",
    "    return true_count / softmax_layer.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194485dc-6526-4fe8-87e2-cb00e6936fb8",
   "metadata": {},
   "source": [
    "### **Visualizing Parameters**\n",
    "\n",
    "We visualize some weight matrices when the neural network is single layer. When there is single layer neural network, the weight matrices one side is equal to total input pixels. The shape of weight matrix is [10][2500]. So everyweight values could have 2500 pixels for each 10 classes. We can easily reshape and make an  grayscale image of weights for each output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a909497d-2cb8-4dd6-933e-ffe266c5dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for visualizing weights of each output class\n",
    "def visualize_weights(parameters):\n",
    "    for i in range(c):\n",
    "        weight_best = parameters[\"w\"][0][i].reshape((img_size, img_size))\n",
    "        plt.imshow(weight_best, cmap='gray', vmin=np.amin(weight_best), vmax=np.amax(weight_best))\n",
    "        plt.savefig(str(i)+\".png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa9ee1-cb6f-459a-b32b-aa6449e1f41b",
   "metadata": {},
   "source": [
    "### **Helper functions for data**\n",
    "\n",
    "Here are some functions for reading from image data and creating proper data structers from them. After that we need to flatten the image data and normalize the values to 0-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba5db111-9989-4f2a-9b8a-54aa83e68895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes all the images urls and their class (which animal)\n",
    "# creates a new csv file which has urls\n",
    "def all_image_urls_to_csv():\n",
    "    folder_names = os.listdir('../Project/raw-img')\n",
    "    category = []\n",
    "    files = []\n",
    "    for k, folder in enumerate(folder_names):\n",
    "        filenames = os.listdir(\"../Project/raw-img/\" + folder)\n",
    "        for file in filenames:\n",
    "            files.append(\"../Project/raw-img/\" + folder + \"/\" + file)\n",
    "            category.append(k)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'filename': files,\n",
    "        'category': category\n",
    "    })\n",
    "    train_df = pd.DataFrame(columns=['filename', 'category'])\n",
    "    for i in range(10):\n",
    "        train_df = train_df.append(df[df.category == i].iloc[:, :])\n",
    "\n",
    "    df.to_csv('out.csv', index=False)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "# \n",
    "def create_data(data, url_category_data, start_index, finish_index):\n",
    "    data = []\n",
    "    for i in range(start_index, finish_index):\n",
    "        path = url_category_data[i][0]\n",
    "        target = url_category_data[i][1]\n",
    "        try:\n",
    "            img_array = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            new_img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "            data.append([new_img_array, target])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "\n",
    "def flatten_and_normalize_data(data, start_index, end_index):\n",
    "    for i in range(start_index, end_index):\n",
    "        arr = np.asarray(data[i][0])\n",
    "        data[i][0] = arr.reshape((img_size * img_size,)).astype(np.float32)\n",
    "        data[i][0] = (data[i][0] / 255)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f538a-d6b7-4f38-b1b3-a0d8de80fca0",
   "metadata": {},
   "source": [
    "### **Train and Validation**\n",
    "\n",
    "In these part of the code, we first train and then validate data for each epoch. In each epoch we creaye mini-batches of changing size(16-32-64-128) and train the parameters with those mini-batches.  After all batches in train data is completed once, one epoch is finished.\n",
    "After one epoch, validation data is used for measuring the performance after training that epoch. When the validation data has the best performance, we keep that parameters to use in the test set. The reason we do that is to deal with overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8870015d-47fa-4713-98b1-9843685517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_amd_validation(x_train, y_train, x_validation, y_validation, url_category_data, learning_rate):\n",
    "    parameters = init_parameters()\n",
    "    cost_list = []\n",
    "    validation_cost_list = []\n",
    "    start_learning_rate = learning_rate\n",
    "\n",
    "    best_parameters_from_validation = parameters\n",
    "    best_performance_from_validation = 0\n",
    "    train_performance_list = []\n",
    "    validation_performance_list = []\n",
    "\n",
    "    for i in range(epoch):  # epoch\n",
    "        data = []\n",
    "        performance_val_train = 0\n",
    "        for j in range(0, total_train_image_count, batch_size):  # batch\n",
    "            data = create_data(data, url_category_data, j, j + batch_size)\n",
    "            data = flatten_and_normalize_data(data, 0, batch_size)\n",
    "            x_train, y_train = init_train(data, x_train, y_train)\n",
    "\n",
    "            activation_funcs = forward_propagation(x_train, parameters)\n",
    "            cost = cost_function(activation_funcs[-1], y_train)\n",
    "            perf_t = performance(activation_funcs[-1], y_train)\n",
    "            performance_val_train += perf_t\n",
    "            parameters = backward_prop(x_train, y_train, parameters, activation_funcs)\n",
    "\n",
    "        cost_list.append(cost)\n",
    "        performance_val_train = performance_val_train / (total_train_image_count / batch_size)\n",
    "        train_performance_list.append(performance_val_train)\n",
    "        print(\"Cost after\", i, \"epoch is :\", cost)\n",
    "        print(\"------------------------------------------------\")\n",
    "\n",
    "        performance_val = 0\n",
    "        for k in range(total_train_image_count, total_train_image_count + total_validation_image_count, batch_size):\n",
    "            validation_data = []\n",
    "            validation_data = create_data(validation_data, url_category_data, k, k + batch_size)\n",
    "            validation_data = flatten_and_normalize_data(validation_data, 0, batch_size)\n",
    "            x_validation, y_validation = init_train(validation_data, x_validation, y_validation)\n",
    "            activation_funcs_v = forward_propagation(x_validation, parameters)\n",
    "            cost_v = cost_function(activation_funcs_v[-1], y_validation)\n",
    "            perf = performance(activation_funcs_v[-1], y_validation)\n",
    "            performance_val += perf\n",
    "        validation_cost_list.append(cost_v)\n",
    "        print(\"Validation Cost :\", cost_v)\n",
    "        performance_val = performance_val / (total_validation_image_count / batch_size)\n",
    "        validation_performance_list.append(performance_val)\n",
    "        print(\"Performance : \", performance_val)\n",
    "        if performance_val >= best_performance_from_validation:\n",
    "            best_performance_from_validation = performance_val\n",
    "            print(\"BEST PERFORMANCE FOR NOW : \", best_performance_from_validation)\n",
    "            best_parameters_from_validation = parameters\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "        learning_rate *= 0.95\n",
    "\n",
    "    plt.title(\n",
    "        \"TRAIN DATA \\n\" + \"Hidden Layer Count:\" + str(hidden_layer_count) + \"-- Learning Rate:\" + str(\n",
    "            start_learning_rate) + \"-- Batch:\" + str(batch_size))\n",
    "    plt.plot(np.arange(0, len(cost_list)), cost_list, label=\"test\")\n",
    "    plt.plot(np.arange(0, len(validation_cost_list)), validation_cost_list, label=\"validation\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"cost.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"VALIDATION DATA \\n\" + \"Hidden Layer Count:\" + str(hidden_layer_count) + \"-- Learning Rate:\" + str(\n",
    "        start_learning_rate) + \"-- Batch:\" + str(batch_size))\n",
    "    plt.plot(np.arange(0, len(train_performance_list)), train_performance_list, label=\"train\")\n",
    "    plt.plot(np.arange(0, len(validation_performance_list)), validation_performance_list, label=\"validation\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"performance.png\")\n",
    "    plt.show()\n",
    "    return best_parameters_from_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d044344e-25b4-4eeb-b012-4b38dcf486e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(learning_rate=learning_rate):\n",
    "    train_df = all_image_urls_to_csv()\n",
    "    url_category_data = np.array(train_df.iloc[:, :])\n",
    "    np.random.seed(101)\n",
    "    np.random.shuffle(url_category_data)\n",
    "\n",
    "    x_train = np.zeros((n, batch_size))\n",
    "    y_train = np.zeros((c, batch_size))\n",
    "\n",
    "    x_validation = np.zeros((n, batch_size))\n",
    "    y_validation = np.zeros((c, batch_size))\n",
    "\n",
    "    x_test = np.zeros((n, batch_size))\n",
    "    y_test = np.zeros((c, batch_size))\n",
    "\n",
    "    parameters = train_amd_validation(x_train, y_train, x_validation, y_validation, url_category_data, learning_rate)\n",
    "    np.save(\"parameters.npy\", parameters)\n",
    "\n",
    "    test_data = []\n",
    "    performance_test = 0\n",
    "    start = total_train_image_count + total_validation_image_count\n",
    "    for j in range(start, start + total_test_image_count, batch_size):\n",
    "        test_data = create_data(test_data, url_category_data, j, j + batch_size)\n",
    "        test_data = flatten_and_normalize_data(test_data, 0, batch_size)\n",
    "        x_test, y_test = init_train(test_data, x_test, y_test)\n",
    "\n",
    "        activation_funcs_t = forward_propagation(x_test, parameters)\n",
    "        perf = performance(activation_funcs_t[-1], y_test)\n",
    "        performance_test += perf\n",
    "    performance_test = performance_test / (total_test_image_count / batch_size)\n",
    "    print(\"TEST PERFORMANCE\", performance_test)\n",
    "\n",
    "    if hidden_layer_count == 0:\n",
    "        visualize_weights(parameters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7ead0-ecda-4df2-b705-03435f9340c4",
   "metadata": {},
   "source": [
    "# **Analysis of Part1**\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643925f-7b4b-4a2d-b742-f849189b4dbf",
   "metadata": {},
   "source": [
    "For analysis of part1 we change some parameters and try to observe the outcomes. Here is some example graph. We set the parameters as:\n",
    "\n",
    "|  | Values | \n",
    "| --- | --- |\n",
    "| Hidden Layer Count | 1 |\n",
    "| Activation Function |  tanh |\n",
    "| Batch Size | 32 |\n",
    "| Learning Rate | 0.02 |\n",
    "| Accuracy | 32,17 |\n",
    "\n",
    "Our accuracy in the trials are not so high and we think this is because of low epoch counts we use because we can see the increasing performance and decreasing cost between epochs. So we think if our computers are little bit better and we can run more epochs in suitable timeline, we can see higher accuracy rates. There is an example graph.\n",
    "\n",
    "P.S.= I accidentally write \"test\" on the label of the train data sets cost graph. In all graphs, when you see the word \"test\", we tried to mean \"train\".\n",
    "\n",
    "<img src=./hiddenlayer_1/tan/b32_lr0,02/cost.png width=400>\n",
    "\n",
    "As we can see the cost of test set and train set is decreasing simultaneously as we train the neural network. We use validation to choose best parameters because we expect overfitting with train data. But in this case we didn't see any overfitting, we think it is because of low epoch count. If we keep training, parameters should be overfit in some point. But besides that, we can see that our cost is decreasing properly, therefore the performance is increasing. You can see performance graph below.\n",
    "\n",
    "<img src=./hiddenlayer_1/tan/b32_lr0,02/performance.png width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fcdbd7-a3f7-4efc-869e-ae473009c983",
   "metadata": {},
   "source": [
    "### **1. Effects of Changing Hidden Layer**\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac661f0d-01f7-4794-8ca9-9646cf7af57b",
   "metadata": {},
   "source": [
    "* **Single Layer Neural Network**\n",
    "\n",
    "In single layer neural networks, there is only one inout layer and one output layer. The activation function for output layer is softmax function. As we can see from the cost and performance graphs, while the cost of train data set is constanly decreasing, the performance increases. And also from the cost graphic, we see the validation set's cost, it first decreases but at some point it started to increase. That point could be the time when overfitting starts. In our implementation when overfitting starts, we store the best parameters for test set. So when overfitting starts, our paraneters stops updating actually. So in this example the best parameters are selected from epoch 5 to 10 where the overfitting just starts.\n",
    "\n",
    "| | Accuracy|\n",
    "| -- | -- |\n",
    "| 0 hidden layer test set | 26,27|\n",
    "\n",
    "<img src=./hiddenlayer_0/b32_lr0,01/cost.png width=300> <img src=./hiddenlayer_0/b32_lr0,01/performance.png width=300>\n",
    "\n",
    "\n",
    "* **1 Layer Neural Network**\n",
    "\n",
    "In 1 layer neural networks, there is one input layer, 1 hidden layer and one output layer. The activation function for hidden layer is tanh function and for output it is softmax function. As we can see from the cost and performance graphs, while the cost of train data set is constanly decreasing, the performance increases\n",
    "\n",
    "| | Accuracy|\n",
    "| -- | -- |\n",
    "| 1 hidden layer test set | 29,11 |\n",
    "\n",
    "<img src=./hiddenlayer_1/tan/b32_lr0,01/cost.png width=300> <img src=./hiddenlayer_1/tan/b32_lr0,01/performance.png width=300>\n",
    "\n",
    "* **2 Layer Neural Network**\n",
    "\n",
    "In 2 layer neural networks, there is one input layer, 2 hidden layers and one output layer. The activation function for hidden layers are tanh function and for output it is softmax function. From the graphs we see that there wasn't much change between the epoch after epoch5. We think that is a problem.\n",
    "\n",
    "| | Accuracy|\n",
    "| -- | -- |\n",
    "| 2 hidden layer test set | 18,25 |\n",
    "\n",
    "<img src=./hiddenlayer_2/tan/b32_lr0,02/cost.png width=300> <img src=./hiddenlayer_2/tan/b32_lr0,02/performance.png width=300>\n",
    "\n",
    "**Overall**\n",
    "As we can see, the performance of 1 hidden layer is the best one. After that single neural network works well too. We think that may be because when there is one hidden layer, it was perfectly learn what it needs to learn.In 1 hidden layer situation our neural network works well and learn more than the others. Most of the best accuracy vvalues are come from 1 hidden layer trials. And when there is more layers, that could fall into the problem of overfitting. And also no hidden layer approach is not a bad approach for our case ı think. It directky classifies by the weights and biasses that learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f44b6-1005-40b1-9467-23642a30a1af",
   "metadata": {},
   "source": [
    "### **2. Effects of Activation Functions**\n",
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d5250-5977-4787-8776-7404c6e64ee6",
   "metadata": {},
   "source": [
    "* **Sigmoid**    $y=\\frac{1}{1+ e^(-x)}$\n",
    "* **Tanh**       $y=tanh(x)$\n",
    "* **Relu**       $y=max(0,x)$\n",
    "\n",
    "This time we create our networks with different activation functions in hidden layers. Below you can see the accuracy of each neural network and also cost/performance graphics of each trial. In the trials everything is constant but the activation functions.\n",
    "\n",
    "Graphics are given with this order: \n",
    "1. 1 Layer, Batch Size=64 , Learning Rate=0.01, Activation Function = Sigmoid\n",
    "2. 1 Layer, Batch Size=64 , Learning Rate=0.01, Activation Function = Tanh\n",
    "3. 1 Layer, Batch Size=64 , Learning Rate=0.01, Activation Function = Relu\n",
    "\n",
    "The accuracy values of each trial:\n",
    "\n",
    "| Hidden Layer Count |Activation Function| Learning Rate | Batch Size | **Test Accuracy** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1  | sigmoid |0.01 | 32 | 22 |\n",
    "| 1  | tanh |0.01 | 32 | 26,56 |\n",
    "| 1  | relu |0.01 | 32 | 29,50 |\n",
    "\n",
    "\n",
    "\n",
    "**Cost Graphics of Trials**\n",
    "\n",
    "1.<img src=./hiddenlayer_1/sigmoid/b32_lr0,01/cost.png width=300> 2. <img src=./hiddenlayer_1/tan/b64_lr0,01/cost.png  width=300>              \n",
    "3. <img src=./hiddenlayer_1/relu/b64_lr0,01/cost.png  width=300> \n",
    "\n",
    "**Performance Graphics of Trials**\n",
    "\n",
    "1.<img src=./hiddenlayer_1/sigmoid/b32_lr0,01/performance.png width=300> 2. <img src=./hiddenlayer_1/tan/b64_lr0,01/performance.png  width=300>              \n",
    "3. <img src=./hiddenlayer_1/relu/b64_lr0,01/performance.png  width=300> \n",
    "\n",
    "\n",
    "As we can see from the graphs and accuracy values, Relu is the best and then tanh. The worst activation function amongst them is sigmoid. In sigmoid function and tanh function there is vanishing gradient problem. In those functions the problem arises when a large input space is mapped to a small one, causing the derivatives to disappear. Because of that sigmoid and tanh functions doesn't work well in our dataset.\n",
    "\n",
    "The advantages of the relu function is, maximum Threshold values are infinity, so there is no issue of Vanishing Gradient problem so the output prediction accuracy and there efficiency is maximum an speed is fast compare to other activation functions.\n",
    "\n",
    "The performances in our trials, support those issues. We can say relu is the best activation function for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da895e8e-1a21-4ee5-a0a1-c0c36dbf15bc",
   "metadata": {},
   "source": [
    "### **3. Effect of Changing Batch Size**\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "For a one layer neural network, we create our networks with different batch sizes. Below you can see the accuracy of each neural network and also cost/performance graphics of each trial.\n",
    "In the trials everything is constant but the batch size. The batch sizes changes from the values 16-32-64-128. \n",
    "\n",
    "Graphics are given with this order: \n",
    "1. 1 Layer, Learning Rate=0.01, Batch Size=16\n",
    "2. 1 Layer, Learning Rate=0.01, Batch Size=32\n",
    "3. 1 Layer, Learning Rate=0.01, Batch Size=64\n",
    "4. 1 Layer, Learning Rate=0.01, Batch Size=128\n",
    "\n",
    "The accuracy values of each trial:\n",
    "\n",
    "| Hidden Layer Count |Activation Function| Learning Rate | Batch Size | **Test Accuracy** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1  | tanh |0.01 | 16 | 31 |\n",
    "| 1  | tanh |0.01 | 32 | 29 |\n",
    "| 1  | tanh |0.01 | 64 | 26 |\n",
    "| 1  | tanh |0.01 | 128 | 21 |\n",
    "\n",
    "\n",
    "\n",
    "**Cost Graphics of Trials**\n",
    "\n",
    "1.<img src=./hiddenlayer_1/tan/b16_lr0,01/cost.png width=300> 2. <img src=./hiddenlayer_1/tan/b32_lr0,01/cost.png  width=300>              \n",
    "3. <img src=./hiddenlayer_1/tan/b64_lr0,01/cost.png  width=300> 4. <img src=./hiddenlayer_1/tan/b128_lr0,01/cost.png  width=300>\n",
    "\n",
    "\n",
    "**Performance Graphics of Trials**\n",
    "\n",
    "\n",
    "1.<img src=./hiddenlayer_1/tan/b16_lr0,01/performance.png width=300> 2. <img src=./hiddenlayer_1/tan/b32_lr0,01/performance.png  width=300>              \n",
    "3. <img src=./hiddenlayer_1/tan/b64_lr0,01/performance.png  width=300> 4. <img src=./hiddenlayer_1/tan/b128_lr0,01/performance.png  width=300>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60d76e-f8e9-4b11-bbf6-11c2a31b1f80",
   "metadata": {},
   "source": [
    "As we can see from the accuracy values of each trial, when the batch size is increasing, our performance is increasing a little bit too. We think this situation happens becuase, in lower batch sizes there were more iterations in each epoch. For example, we did out trials with 30 epoch. In batch size 16, there will be 1280 iterations in each epoch. And in batch size 128 there will be 160 iterations in each epoch. That means in lower batch sizes there will be more updates in each epoch. Since each update aims to find local minima of that value with gradient descent, if we do more updates with proper rates we can get to local minima more quickly. Eventhough small batch sizes have more iteration and more time complexity, they has slightly better performance.\n",
    "\n",
    "From the graphs we can see that smaller batch sizes has more smooth increasing of performance. And also for batch size=16 we can see a little overfitting around the epoch 15. We solve this prolem by choosing he best parameters by validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a0ae6-5301-4d1e-9ee9-aefc4205e07b",
   "metadata": {},
   "source": [
    "### **4. Effect of Changing Learning Rate**\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15a092-5340-4e46-bda4-185a440d6ef9",
   "metadata": {},
   "source": [
    "This time we create our networks with different learning rates. Below you can see the accuracy of each neural network and also cost/performance graphics of each trial. In the trials everything is constant but the learning  rates. The learning rates changes from the values 0,02 - 0,01 - 0,005. \n",
    "\n",
    "Graphics are given with this order: \n",
    "1. 1 Layer, Batch Size=32 , Learning Rate=0.02\n",
    "2. 1 Layer, Batch Size=32 , Learning Rate=0.01\n",
    "3. 1 Layer, Batch Size=32 , Learning Rate=0.005\n",
    "\n",
    "The accuracy values of each trial:\n",
    "\n",
    "| Hidden Layer Count |Activation Function| Learning Rate | Batch Size | **Test Accuracy** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1  | tanh |0.02 | 32 | 32 |\n",
    "| 1  | tanh |0.01 | 32 | 29 |\n",
    "| 1  | tanh |0.005 | 32 | 27 |\n",
    "\n",
    "\n",
    "\n",
    "**Cost Graphics of Trials**\n",
    "\n",
    "1.<img src=./hiddenlayer_1/tan/b32_lr0,02/cost.png width=300> 2. <img src=./hiddenlayer_1/tan/b32_lr0,01/cost.png  width=300>              \n",
    "3. <img src=./hiddenlayer_1/tan/b32_lr0,05/cost.png  width=300> \n",
    "\n",
    "**Performance Graphics of Trials**\n",
    "\n",
    "1.<img src=./hiddenlayer_1/tan/b32_lr0,02/performance.png width=300> 2. <img src=./hiddenlayer_1/tan/b32_lr0,01/performance.png  width=300>              \n",
    "3. <img src=./hiddenlayer_1/tan/b32_lr0,05/performance.png  width=300> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8f3f5-26c3-4257-9c5b-a3e8fdd64f92",
   "metadata": {},
   "source": [
    "As we can see from the accuracy values, bigger learning rate has better performance. Actually we know that if the learning rate is too high, the gradient descent can overshoot the minimum and it may fail to converge. But also if the learning rate is too small, gradient descent is small and it takes so much time, iterations to reach miinimum. Both ways is bad for us. \n",
    "As we can see from the trials, the best performance is when the learning rate is 0,02 which is highest in our trials. It means it is not too high for our data. But from the cost graphics we can say that learning rate 0,005 is too small because in 30 epochs the cost is not decreases as much as we wanted to.\n",
    "From our outputs and graphs we say that learning rate 0,02 is the better choice amongst others. It has better capability to reduce cost in more faster way and it does noy misses any minimum as we can see. And for preventing this we also reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb9f6f-8d08-453e-8e6a-d86129c8808b",
   "metadata": {},
   "source": [
    "## **Visualizing Parameters**\n",
    "\n",
    "Weight matrix of **butterfly** output class:\n",
    "\n",
    "<img src=./visualization/4.png width=300> \n",
    "\n",
    "\n",
    "Weight matrix of **elephant** output class:\n",
    "\n",
    "<img src=./visualization/6.png width=300> \n",
    "\n",
    "Weight matrix of **chicken** output class:\n",
    "\n",
    "<img src=./visualization/5.png width=300> \n",
    "\n",
    "Weight matrix of **spider** output class:\n",
    "\n",
    "<img src=./visualization/2.png width=300> \n",
    "\n",
    "Those images of weight matrices are taken from single layer neural networks. The reason behind this is tho get an same image size with the weight matrix. And also images are more precise when de the epoch size is decreasing. If the epoch size is too high most of the images looks like this:\n",
    "\n",
    "A butterfly when epoch is much higher:\n",
    "\n",
    "<img src=./hiddenlayer_0/b32_lr0,01/4.png width=300> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83e822-58fc-4a33-80db-6160982488e2",
   "metadata": {},
   "source": [
    "# **PART 2**\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16066c-2a95-4d9b-97cf-6d2d96f77d75",
   "metadata": {},
   "source": [
    "### **Convulational Neural Network**\n",
    "\n",
    "Convolutional Neural Network (CNN) is an neural network which extracts or identifies a feature in a particular image. CNN has the following five basic components:\n",
    "\n",
    "* **Convolution** : to detect features in an image\n",
    "* **ReLU** : to make the image smooth and make boundaries distinct\n",
    "* **Pooling** : to help fix distored images\n",
    "* **Flattening** : to turn the image into a suitable representation\n",
    "* **Full connection** : to process the data in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c74e9-8353-4f82-8939-fb15e7309928",
   "metadata": {},
   "source": [
    "### **VGG-19**\n",
    "\n",
    "In this part of the assignment, we used pretrained VGG-19 convolutional neural network (CNN) and finetune this network to classify the sample images.\n",
    "Here is the the figure of VGG-19 network architecture:\n",
    "<img src=https://ichi.pro/assets/images/max/724/0*E6BE6GDv-53smX0B.jpg wifth=400>\n",
    "\n",
    "\n",
    "So in simple language VGG is a deep CNN used to classify images.It is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer). The layers in VGG19 model are as follows:\n",
    "\n",
    "* Conv3x3 (64)\n",
    "* Conv3x3 (64)\n",
    "* MaxPool\n",
    "* Conv3x3 (128)\n",
    "* Conv3x3 (128)\n",
    "* MaxPool\n",
    "* Conv3x3 (256)\n",
    "* Conv3x3 (256)\n",
    "* Conv3x3 (256)\n",
    "* Conv3x3 (256)\n",
    "* MaxPool\n",
    "* Conv3x3 (512)\n",
    "* Conv3x3 (512)\n",
    "* Conv3x3 (512)\n",
    "* Conv3x3 (512)\n",
    "* MaxPool\n",
    "* Conv3x3 (512)\n",
    "* Conv3x3 (512)\n",
    "* Conv3x3 (512)\n",
    "* Conv3x3 (512)\n",
    "* MaxPool\n",
    "* Fully Connected (4096)\n",
    "* Fully Connected (4096)\n",
    "* Fully Connected (1000)\n",
    "* SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfb1a7-d390-4603-8d5c-961038be3f29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
